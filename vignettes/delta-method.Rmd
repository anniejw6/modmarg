---
title: "What is the Delta Method?"
author: "Alex Gold, Annie Wang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

There are a number of possible ways to compute the standard errors for 
[margins](usage.html) of a regression. If you're a total masochist, it might be possible to derive a 
probability density function for the margin itself, but that's surely a huge pain
and might not even exist. It is also possible to use simulation or bootstrapping
to create standard errors for the margin. 

In this package, we follow 
[Stata's margins command](https://www.stata.com/help.cgi?margins) and use the
Delta Method, which is a semi-parametric method that takes advantage of closed form solution to $\frac{d(\text{link}^{-1}(X \beta))}{d(X \beta)}$ to improve computational time relative to simulation or bootstrap methods.

# The Delta Method
The Delta Method is a general method for deriving the variance of a function of random variables with known variance. In this case, the Delta Method takes advantage of the fact that the margin is (usually) an infinitely differentiable function of the data, $X$, and the vector of $\beta$s to derive a closed-form solution for the standard errors of the margin.^[Usually because this statement requires that the canonical link function for the regression has a closed-form derivative. Luckily, this is true for most common forms of linear regression.] 

In particular, the delta method uses a [Taylor Series](https://en.wikipedia.org/wiki/Taylor_series) expansion of the inverse link function of the regression to approximate the margin in the neighborhood of $X$ and the $\beta$s and derive variation near that point.

## A reminder about Taylor Series
The [Taylor Expansion](https://en.wikipedia.org/wiki/Taylor_series) is a useful tool because it allows us to restate a differentiable function, $G(x)$, in terms of (an infinite sum of) the derivatives of $G(x)$. To be more precise, an infinitely differentiable $G(x)$ evaluated at $a$ can be written as
$$G(x) = G(a) + \frac{G'(a)}{1!}(x - a) + \frac{G''(a)}{2!}(x-a)^2 + 
\frac{G'''(a)}{3!}(x-a)^3 + \dots$$

If we cut off the expansion after some number of terms (two is commmon), we can get a useful 
approximation of $G(x)$.

## Taylor Series and the Delta Method
In the case of predictive margins (levels), where the regression model has [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function) $\text{link}$^[The exact form of the link function and its inverse will depend on the type of regression. For example, the logit function is the canonical link function for logistic regression and allows transformations between probabilities and log-odds.], the column vector of *predictive margins* $P_m$ at covariates $X_1$ is 
$$P_m(X_1 \beta) = \text{link}^{-1}(X_1 \beta)$$

For the *predicted effects* of that same regression, the column vector of predicted effects $P_e$ is a function of $X \beta$ such that 

$$P_e(X_1 \beta) = \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}$$ 

Depending on whether the effect is over a continuous or categorical variable, this may be an actual derivative (the instantaneous rate of change) or the subtraction of $P(X \beta)$ calculated at one value of $X$ from another (the first difference). 

Using the Taylor expansion, we can express $P$, a function of the random variable $X \beta$ around the point $X_1 \beta$^[Note that we treat the input $X$ as fixed and $\beta$ as a random variable.], as

$$P(X \beta) = P(X_1 \beta) + 
\frac{d(P(X_1 \beta))}{d(X\beta)}(X\beta - X_1 \beta)$$

Substituting in for the different margins we'll care about, for **predicted margins** (levels), the Taylor expansion is 

$$P_m(X \beta) = \text{link}^{-1}(X_1 \beta) + 
\frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}(X\beta - X_1 \beta)$$

In the case that we are trying to estimate **predicted marginal effects of categorical variables**, we are trying to estimate the effects at $P_e(X_1 \beta - X_2 \beta)$, which gives us

$$ \begin{aligned}
P_e(X \beta) &= \text{link}^{-1} (X_1 \beta - X_2 \beta) + 
\frac{d(\text{link}^{-1}(X_1 \beta - X_2 \beta))}{d(X \beta)}(X\beta - (X_1 \beta - X_2 \beta)) \\
% &= \text{link}^{-1} (X_1 \beta) - \text{link}^{-1} (X_2 \beta) + \frac{d(\text{link}^{-1})}{d(X \beta)}(X_1) - \frac{d(\text{link}^{-1})}{d(X \beta)}(X_2)
\end{aligned}
$$

For **continuous variables, the marginal effect** is a derivative, so 
$$\begin{aligned}
P_e(X_1 \beta) &= \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)} + 
\frac{d^2(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}(\frac{d(\text{link}^{-1}(X \beta))}{d(X \beta)} - \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)})
\end{aligned}
$$

# How does the Delta Method Work?
Broadly speaking, we'll use a bunch of matrix algebra to run the variance/covariance matrix 
from the original regression through the gradient of the inverse link function to
get the variance of the margin. 

In particular, we’ll use the “sandwich” method of deriving the variance of a function of a vector of random variables. In this case, we'll multiply the partial derivatives of the inverse link function by the original variance/covariance matrix from the regression.^[Quick reminder on the sandwich method: for scalar $a$ and $r$, where $b$ is the variance of random variable $r$, $Var(a*r)$ is $a^2*b$. The matrix analog of $a^2*Var(b)$ is $ABA^T$.]

Keeping in mind that the margin is given by $\text{link}^{-1}(X \beta)$, all of the first partial derivatives of the margin are given by the the Jacobian matrix.^[Since the Jacobian matrix is just the name of the matrix of all first partial derviatives of a vector-valued function.] For any predicted level indexed by $i$ in a regression, the $i,j$th element of the jacobian will be the derivative of predicted level $i$ with respect to regressor $j$. 

We'll multiply that Jacobian by the variance-covariance matrix of the regression, which captures the variance structure of the coefficients. As a quick reminder about the variance-covariance matrix, if your model contains coefficients $b_0$ to $b_n$ (each with mean $\mu_{b_i}$ and standard deviation $\sigma_{b_i}$), the $i,j$th element of the variance-covariance matrix is $cov(b_i, b_j)$.

Now that we're all clear on how the pieces fit together, [Wikipedia contains a really nice derivation](https://en.wikipedia.org/wiki/Delta_method#Multivariate_delta_method)
of the multivariate delta method.

# Step-By-Step

1. Calculate the Jacobian matrix of the inverse link function of $X \beta$, $J$.

2. Get the variance-covariance matrix, $V$, from the regression output, or calculate it some other way.^[Usually you'll just want the variance-covariance matrix of the regression, but you'll need to modify the vanilla variance-covariance matrix if you want to cluster standard errors or something.]

3. Sandwich multiply the matrices: $J^{T} \times V \times J$. You’ll end up with a $k \times 1$ matrix for the $k$ predicted levels/effects.

## Calculating the Jacobian

### Predictive Levels 

#### Categorical or Continuous Variables

1. Create the covariates at which you would like predicted levels. Call these matrices $A_1, A_2, ..., A_k$. 

1. Calculate predicted levels for each of these matrices, call each of the resulting 1 x n column vectors $q_1, q_2, ..., q_k$.

1. Calculate the derivative of the link function with respect to $X \beta$ (e.g for a simple logit with predicted level $p$, $p \cdot (1-p)$). 

1. Run each vector of predicted levels through the scalar-valued link function to end up with another 1 x n column vector. Call this $q^*_i$

1. Take the cross product of each $q^*_i$ and $A_i$ to get a row vector. Divide by the number of observations in $A_i$.

1. Row bind your vectors together. This is your Jacobian.

### Predictive Effects

The big change from the section above is that you are now calculating the variance on predictive effects, so you need to take the second derivative of the link function. (Because the first derivative is the effect.)

#### Categorical Variables

Take the second derivative by subtracting each of the levels from the base level and return that as the jacobian. 

#### Continuous Variables

Compute the second derivative of the link function and use that in place of the first derivative above. You need to explicitly compute the second derivative because you want an instantaneous rate of change as opposed to the rate of change over a range as with categorical variables above.

# Example

Say we're fitting a logistic regression using the `margex` data, and we're interested in the predicted outcome for different treatment groups:

```{r}
library(modmarg)
data(margex)
lg <- glm(outcome ~ treatment * age, data = margex, family = 'binomial')
summary(lg)
```

where the `treatment` variable $\tau \in {0, 1}$. What is the average outcome when $\tau = 0$ or $\tau = 1$?

Based on these results alone, we don't really know what the treatment effect is, both because we can't directly interpret the coefficients, because there's an interaction term, and because the effect of each covariate depends on the levels of the other covariates.

## Point estimates

To get an estimate of the average levels of the outcome for different values of treatment, we can set treatment to 0 or 1 for the entire dataset, generate predicted outcomes for the dataset, and averaging the results (in other words, what would the outcome have been, if everybody was in the control / treatment group?). We'll use the `predict` and `plogis` functions to do this. Predict will calculate the linear predictors (in this case, setting $\tau = t$: 
\[
\hat{y}_i = \alpha + \beta_1 \cdot t + \beta_2 \cdot \text{age}_i + \beta_3 \cdot (t \cdot \text{age}_i)
\]
`plogis` is the inverse link function $f(z) = \frac{1}{1 + \exp(-z)}$, which transforms the linear predictors to the scale of the outcome variable (predicted probabilities). Finally, we can find the average predicted level.

Combining these, we have:
\[
g(\hat{y}_{\tau = t}) = \frac{1}{n} \sum_{i = 1}^n (1 + \exp(-\beta x_{i, \tau = t}))^{-1}
\]
We'll suppress the $\tau = t$ subscripts from here on.

Now let's do this math in R: set treatment to 0 or 1, generate linear predictors, transform them to predicted outcomes, and take the mean.

```{r}
# Extract data from the model
new <- lg$model
# Set treatment to 0 for all observations
new$treatment <- 0
# Get linear predictors
pred_ctl <- predict(lg, newdata = new)
# Apply the inverse link function and take the mean
mean_pred_ctl <- mean(plogis(pred_ctl))

# Once again, with treatment
new <- lg$model
# Set treatment to 1 for all observations
new$treatment <- 1
# Get linear predictors
pred_treat <- predict(lg, newdata = new)
# Apply the inverse link function and take the mean
mean_pred_treat <- mean(plogis(pred_treat))

print(mean_pred_ctl)
print(mean_pred_treat)
```

## Variance

So far this is pretty straightforward, but we want to know the standard error of these estimates. As explained above, we can approximate their variance: if the gradient $\nabla g(\hat{y})$, the derivative with respect to each $\beta$ parameter, is defined as $J$, then

\[\text{Var}(g(\hat{y})) =  J \times V \times J^\prime\]

\ldots Going back to our function from before, let's start with the coefficient on treatment, $\beta_1$.

\begin{align*}
g(\hat{y}) &= \frac{1}{n} \sum_{i = 1}^n (1 + \exp(-X_i\beta))^{-1}\\
\frac{\partial}{\partial \beta_1} g(\hat{y}) &= \frac{\partial}{\partial \beta_1} \left[\frac{1}{n} \sum_{i = 1}^n (1 + \exp(-X_i\beta ))^{-1}\right]\\
&= \frac{1}{n} \sum_{i = 1}^n \frac{\partial}{\partial \beta_1} (1 + \exp(-X_i\beta))^{-1}\\
&= \frac{1}{n} \sum_{i = 1}^n -(1 + \exp(-X_i\beta))^{-2} \cdot \frac{\partial}{\partial \beta_1} (1 + \exp(-X_i\beta))\\
&= \frac{1}{n} \sum_{i = 1}^n -(1 + \exp(-X_i\beta))^{-2} \cdot \exp(-X_i\beta) \cdot \frac{\partial}{\partial \beta_1} (-X_i\beta)
\end{align*}

Coming up for air for a second: what's the last term in that final expression? $X_i\beta = \alpha + \beta_1 \tau_i + \beta_2 \text{age}_i + \beta_3 (\tau_i \times \text{age}_i)$, so the derivative of that with respect to $\beta_1$ is just $\tau_i$. Therefore, 

\begin{align*}
\frac{\partial}{\partial \beta_1} g(\hat{y}) &= \frac{1}{n} \sum_{i = 1}^n -(1 + \exp(-X_i\beta))^{-2} \cdot \exp(-X_i\beta) \cdot \frac{\partial}{\partial \beta_1} (-X_i\beta)\\
&= \frac{1}{n} \sum_{i = 1}^n -(1 + \exp(-X_i\beta))^{-2} \cdot \exp(-X_i\beta) \cdot (-\tau_i) \\
&= \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i
\end{align*}
\ldots which is the first term of our Jacobian. Basically the chain rule means that inside the sum, we have the derivative of the inverse link function, multiplied by the derviative of the linear predictor (which is just one of the covariates for the model).^[Note that the first term is sometimes written $\frac{1}{1 + \exp(-X_i\beta)} \cdot \frac{1}{1 + \exp(X_i\beta)}$ since that implies $\frac{\partial}{\partial \beta} \text{logit}^{-1}(X\beta) = \text{logit}^{-1}(X\beta) \cdot \text{logit}^{-1}(-X\beta)$]

What about the other terms? Well, because the $X_i\beta$ term is additively separable, they're pretty much the same but with a different term at the end - for example, the derivative with respect to $\beta_2$ is going to be $\frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta))}{(1 + \exp(-X_i\beta))^2} \cdot \text{age}_i$. So the full gradient is
\[
J = 
\left[
  \begin{array}{l} \\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot 1\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \text{age}_i\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i \cdot \text{age}_i
  \end{array}
\right]
\]

In other words, apply the derivative of the inverse link function to the linear predictors, multiply them by one of the covariates, sum that up, and, divide by $n$. This can be written much more succinctly as

\[
J = 
\frac{1}{n}
\left[
  \begin{array} \\
     \frac{\exp(-X_1\beta)}{(1 + \exp(-X_1\beta))^2} &
     \frac{\exp(-X_2\beta)}{(1 + \exp(-X_2\beta))^2} &
     \cdots &
     \frac{\exp(-X_n\beta)}{(1 + \exp(-X_n\beta))^2}
  \end{array}
\right]
\times
\left[
  \begin{array}{cccc}\\
    1 & \tau_1 & \text{age}_1 & \tau_1 \cdot \text{age}_1 \\
    1 & \tau_2 & \text{age}_2 & \tau_2 \cdot \text{age}_2 \\
    \vdots  & \vdots & \vdots & \vdots\\
    1 & \tau_n & \text{age}_n & \tau_n \cdot \text{age}_n \\
  \end{array}
\right]
\]

The first term applies the derivative of the inverse link function to every linear predictor in the model, and the second term is the covariate matrix from our data! How convenient! That's going to be true for all general linear models. So we can rewrite this as
\[J = \frac{\left[\begin{array}\\ \frac{\exp(-X_1\beta)}{(1 + \exp(-X_1\beta))^2} & \frac{\exp(-X_2\beta)}{(1 + \exp(-X_2\beta))^2} & \cdots & \frac{\exp(-X_n\beta)}{(1 + \exp(-X_n\beta))^2}\end{array}\right] \cdot X}{n}\]
Recalling that our variance is $J \cdot V \cdot J^\prime$, and that we've already calculated $X_i\beta$ for all $i$, this is relatively simple to do in R:

```{r}
# Control error
deriv <- exp(-pred_ctl) / (1 + exp(-pred_ctl))^2
x <- model.matrix(lg)
x[, 'treatment'] <- 0
x[, 'treatment:age'] <- x[, 'treatment'] * x[, 'age']
grad <- deriv %*% x / nrow(x)
variance <- grad %*% vcov(lg) %*% t(grad)
se_ctl <- sqrt(diag(variance))

# Treat error
deriv <- exp(-pred_treat) / (1 + exp(-pred_treat))^2
x <- model.matrix(lg)
x[, 'treatment'] <- 1
x[, 'treatment:age'] <- x[, 'treatment'] * x[, 'age']
grad <- deriv %*% x / nrow(x)
variance <- grad %*% vcov(lg) %*% t(grad)
se_treat <- sqrt(diag(variance))

print(se_ctl)
print(se_treat)
```

## Review

OK, we've got our estimates and variance:

```{r}
result <- data.frame(
  Label = c("treatment = 0", "treatment = 1"),
  Margin = c(mean_pred_ctl, mean_pred_treat),
  Standard.Error = c(se_ctl, se_treat)
)

print(result)
```

What do we get from modmarg?

```{r}
marg <- modmarg::mod_marg2(mod = lg, var_interest = 'treatment')
marg[[1]][, c("Label", "Margin", "Standard.Error")]
```

Hooray!



